{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-13T07:07:24.488603Z",
     "iopub.status.busy": "2021-01-13T07:07:24.487780Z",
     "iopub.status.idle": "2021-01-13T07:07:24.493563Z",
     "shell.execute_reply": "2021-01-13T07:07:24.494225Z"
    },
    "papermill": {
     "duration": 0.022849,
     "end_time": "2021-01-13T07:07:24.494394",
     "exception": false,
     "start_time": "2021-01-13T07:07:24.471545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-13T07:07:24.516648Z",
     "iopub.status.busy": "2021-01-13T07:07:24.515968Z",
     "iopub.status.idle": "2021-01-13T07:07:48.487894Z",
     "shell.execute_reply": "2021-01-13T07:07:48.488530Z"
    },
    "papermill": {
     "duration": 23.983889,
     "end_time": "2021-01-13T07:07:48.488735",
     "exception": false,
     "start_time": "2021-01-13T07:07:24.504846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'E:/Server_mantain/web-traffic-time-series-forecasting/train_2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-857e205c889c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#zip.ZipFile('/kaggle/input/web-traffic-time-series-forecasting/train_2.csv.zip').extractall()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'E:/Server_mantain/web-traffic-time-series-forecasting/train_2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\rtx3070\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rtx3070\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rtx3070\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rtx3070\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1043\u001b[0m             )\n\u001b[0;32m   1044\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1045\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rtx3070\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1863\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1864\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rtx3070\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m         \"\"\"\n\u001b[1;32m-> 1357\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rtx3070\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'E:/Server_mantain/web-traffic-time-series-forecasting/train_2.csv'"
     ]
    }
   ],
   "source": [
    "# use only train_2.csv\n",
    "#import zipfile as zip\n",
    "\n",
    "#zip.ZipFile('/kaggle/input/web-traffic-time-series-forecasting/train_2.csv.zip').extractall()\n",
    "train_2 = pd.read_csv('E:/Server_mantain/web-traffic-time-series-forecasting/train_2.csv/train_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T07:07:48.523060Z",
     "iopub.status.busy": "2021-01-13T07:07:48.522293Z",
     "iopub.status.idle": "2021-01-13T07:07:48.555335Z",
     "shell.execute_reply": "2021-01-13T07:07:48.555900Z"
    },
    "papermill": {
     "duration": 0.058566,
     "end_time": "2021-01-13T07:07:48.556052",
     "exception": false,
     "start_time": "2021-01-13T07:07:48.497486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145063, 804)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>2015-07-01</th>\n",
       "      <th>2015-07-02</th>\n",
       "      <th>2015-07-03</th>\n",
       "      <th>2015-07-04</th>\n",
       "      <th>2015-07-05</th>\n",
       "      <th>2015-07-06</th>\n",
       "      <th>2015-07-07</th>\n",
       "      <th>2015-07-08</th>\n",
       "      <th>2015-07-09</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-09-01</th>\n",
       "      <th>2017-09-02</th>\n",
       "      <th>2017-09-03</th>\n",
       "      <th>2017-09-04</th>\n",
       "      <th>2017-09-05</th>\n",
       "      <th>2017-09-06</th>\n",
       "      <th>2017-09-07</th>\n",
       "      <th>2017-09-08</th>\n",
       "      <th>2017-09-09</th>\n",
       "      <th>2017-09-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2NE1_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2PM_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3C_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4minute_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 804 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Page  2015-07-01  2015-07-02  \\\n",
       "0            2NE1_zh.wikipedia.org_all-access_spider        18.0        11.0   \n",
       "1             2PM_zh.wikipedia.org_all-access_spider        11.0        14.0   \n",
       "2              3C_zh.wikipedia.org_all-access_spider         1.0         0.0   \n",
       "3         4minute_zh.wikipedia.org_all-access_spider        35.0        13.0   \n",
       "4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...         NaN         NaN   \n",
       "\n",
       "   2015-07-03  2015-07-04  2015-07-05  2015-07-06  2015-07-07  2015-07-08  \\\n",
       "0         5.0        13.0        14.0         9.0         9.0        22.0   \n",
       "1        15.0        18.0        11.0        13.0        22.0        11.0   \n",
       "2         1.0         1.0         0.0         4.0         0.0         3.0   \n",
       "3        10.0        94.0         4.0        26.0        14.0         9.0   \n",
       "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "   2015-07-09  ...  2017-09-01  2017-09-02  2017-09-03  2017-09-04  \\\n",
       "0        26.0  ...        19.0        33.0        33.0        18.0   \n",
       "1        10.0  ...        32.0        30.0        11.0        19.0   \n",
       "2         4.0  ...         6.0         6.0         7.0         2.0   \n",
       "3        11.0  ...         7.0        19.0        19.0         9.0   \n",
       "4         NaN  ...        16.0        16.0        19.0         9.0   \n",
       "\n",
       "   2017-09-05  2017-09-06  2017-09-07  2017-09-08  2017-09-09  2017-09-10  \n",
       "0        16.0        27.0        29.0        23.0        54.0        38.0  \n",
       "1        54.0        25.0        26.0        23.0        13.0        81.0  \n",
       "2         4.0         7.0         3.0         4.0         7.0         6.0  \n",
       "3         6.0        16.0        19.0        30.0        38.0         4.0  \n",
       "4        20.0        23.0        28.0        14.0         8.0         7.0  \n",
       "\n",
       "[5 rows x 804 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_2.shape)\n",
    "train_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T07:07:48.580885Z",
     "iopub.status.busy": "2021-01-13T07:07:48.580113Z",
     "iopub.status.idle": "2021-01-13T07:07:49.692825Z",
     "shell.execute_reply": "2021-01-13T07:07:49.693374Z"
    },
    "papermill": {
     "duration": 1.127816,
     "end_time": "2021-01-13T07:07:49.693545",
     "exception": false,
     "start_time": "2021-01-13T07:07:48.565729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145063, 803)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill NaN to 0\n",
    "train_2 = train_2.fillna(0)\n",
    "train_2 = train_2.drop('Page', axis = 1)\n",
    "train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T07:07:49.721260Z",
     "iopub.status.busy": "2021-01-13T07:07:49.720597Z",
     "iopub.status.idle": "2021-01-13T07:07:50.711553Z",
     "shell.execute_reply": "2021-01-13T07:07:50.710955Z"
    },
    "papermill": {
     "duration": 1.008063,
     "end_time": "2021-01-13T07:07:50.711670",
     "exception": false,
     "start_time": "2021-01-13T07:07:49.703607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((802, 1), (802, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = train_2.iloc[train_2.shape[0] - 1, :].values\n",
    "\n",
    "# data normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "dataset = scaler.fit_transform(np.reshape(dataset, (-1, 1)))\n",
    "\n",
    "x_train = dataset[0:train_2.shape[1] - 1]\n",
    "y_train = dataset[1:train_2.shape[1]]\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T07:07:50.737489Z",
     "iopub.status.busy": "2021-01-13T07:07:50.736812Z",
     "iopub.status.idle": "2021-01-13T07:07:50.796661Z",
     "shell.execute_reply": "2021-01-13T07:07:50.797167Z"
    },
    "papermill": {
     "duration": 0.075389,
     "end_time": "2021-01-13T07:07:50.797301",
     "exception": false,
     "start_time": "2021-01-13T07:07:50.721912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(641, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.2)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T07:07:50.828776Z",
     "iopub.status.busy": "2021-01-13T07:07:50.828118Z",
     "iopub.status.idle": "2021-01-13T07:07:57.061301Z",
     "shell.execute_reply": "2021-01-13T07:07:57.060306Z"
    },
    "papermill": {
     "duration": 6.253307,
     "end_time": "2021-01-13T07:07:57.061455",
     "exception": false,
     "start_time": "2021-01-13T07:07:50.808148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"WTTSF\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1, 1)]            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 16)                1152      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,169\n",
      "Trainable params: 1,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "def build_model():\n",
    "    x = Input(shape = (x_train.shape[1], x_train.shape[2]))\n",
    "    out = x\n",
    "    \n",
    "    out = LSTM(16, activation = 'relu')(out)\n",
    "    out = Dense(1, activation = 'sigmoid')(out)\n",
    "\n",
    "    model = Model(x, out, name = 'WTTSF')\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T07:07:57.091206Z",
     "iopub.status.busy": "2021-01-13T07:07:57.090324Z",
     "iopub.status.idle": "2021-01-13T07:07:57.093531Z",
     "shell.execute_reply": "2021-01-13T07:07:57.092911Z"
    },
    "papermill": {
     "duration": 0.020659,
     "end_time": "2021-01-13T07:07:57.093652",
     "exception": false,
     "start_time": "2021-01-13T07:07:57.072993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('check.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T07:07:57.130859Z",
     "iopub.status.busy": "2021-01-13T07:07:57.126524Z",
     "iopub.status.idle": "2021-01-13T07:08:10.968223Z",
     "shell.execute_reply": "2021-01-13T07:08:10.967093Z"
    },
    "papermill": {
     "duration": 13.862673,
     "end_time": "2021-01-13T07:08:10.968354",
     "exception": false,
     "start_time": "2021-01-13T07:07:57.105681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "61/81 [=====================>........] - ETA: 0s - loss: 0.2255 - acc: 0.7234\n",
      "Epoch 00001: val_acc improved from -inf to 0.71429, saving model to check.h5\n",
      "81/81 [==============================] - 0s 5ms/step - loss: 0.2234 - acc: 0.7363 - val_loss: 0.2100 - val_acc: 0.7143\n",
      "Epoch 2/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.1956 - acc: 0.7423\n",
      "Epoch 00002: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1926 - acc: 0.7363 - val_loss: 0.1751 - val_acc: 0.7143\n",
      "Epoch 3/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.1571 - acc: 0.7239\n",
      "Epoch 00003: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1547 - acc: 0.7363 - val_loss: 0.1340 - val_acc: 0.7143\n",
      "Epoch 4/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.1164 - acc: 0.7239\n",
      "Epoch 00004: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1133 - acc: 0.7363 - val_loss: 0.0926 - val_acc: 0.7143\n",
      "Epoch 5/200\n",
      "62/81 [=====================>........] - ETA: 0s - loss: 0.0789 - acc: 0.7339\n",
      "Epoch 00005: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0759 - acc: 0.7363 - val_loss: 0.0588 - val_acc: 0.7143\n",
      "Epoch 6/200\n",
      "64/81 [======================>.......] - ETA: 0s - loss: 0.0510 - acc: 0.7266\n",
      "Epoch 00006: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0484 - acc: 0.7363 - val_loss: 0.0363 - val_acc: 0.7143\n",
      "Epoch 7/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.0315 - acc: 0.7369\n",
      "Epoch 00007: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0312 - acc: 0.7363 - val_loss: 0.0228 - val_acc: 0.7143\n",
      "Epoch 8/200\n",
      "68/81 [========================>.....] - ETA: 0s - loss: 0.0222 - acc: 0.7390\n",
      "Epoch 00008: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0213 - acc: 0.7363 - val_loss: 0.0151 - val_acc: 0.7143\n",
      "Epoch 9/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.0165 - acc: 0.7500\n",
      "Epoch 00009: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0155 - acc: 0.7363 - val_loss: 0.0105 - val_acc: 0.7143\n",
      "Epoch 10/200\n",
      "68/81 [========================>.....] - ETA: 0s - loss: 0.0125 - acc: 0.7316\n",
      "Epoch 00010: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0121 - acc: 0.7363 - val_loss: 0.0077 - val_acc: 0.7143\n",
      "Epoch 11/200\n",
      "68/81 [========================>.....] - ETA: 0s - loss: 0.0107 - acc: 0.7592\n",
      "Epoch 00011: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0100 - acc: 0.7363 - val_loss: 0.0059 - val_acc: 0.7143\n",
      "Epoch 12/200\n",
      "69/81 [========================>.....] - ETA: 0s - loss: 0.0091 - acc: 0.7337\n",
      "Epoch 00012: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0086 - acc: 0.7363 - val_loss: 0.0047 - val_acc: 0.7143\n",
      "Epoch 13/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.0084 - acc: 0.7313\n",
      "Epoch 00013: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0076 - acc: 0.7363 - val_loss: 0.0039 - val_acc: 0.7143\n",
      "Epoch 14/200\n",
      "69/81 [========================>.....] - ETA: 0s - loss: 0.0059 - acc: 0.7446\n",
      "Epoch 00014: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0070 - acc: 0.7363 - val_loss: 0.0033 - val_acc: 0.7143\n",
      "Epoch 15/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0054 - acc: 0.7385\n",
      "Epoch 00015: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0065 - acc: 0.7363 - val_loss: 0.0028 - val_acc: 0.7143\n",
      "Epoch 16/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.0066 - acc: 0.7351\n",
      "Epoch 00016: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0062 - acc: 0.7363 - val_loss: 0.0025 - val_acc: 0.7143\n",
      "Epoch 17/200\n",
      "69/81 [========================>.....] - ETA: 0s - loss: 0.0064 - acc: 0.7518\n",
      "Epoch 00017: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0059 - acc: 0.7363 - val_loss: 0.0023 - val_acc: 0.7143\n",
      "Epoch 18/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0047 - acc: 0.7404\n",
      "Epoch 00018: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0057 - acc: 0.7363 - val_loss: 0.0021 - val_acc: 0.7143\n",
      "Epoch 19/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0063 - acc: 0.7250\n",
      "Epoch 00019: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0055 - acc: 0.7363 - val_loss: 0.0019 - val_acc: 0.7143\n",
      "Epoch 20/200\n",
      "69/81 [========================>.....] - ETA: 0s - loss: 0.0040 - acc: 0.7464\n",
      "Epoch 00020: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0054 - acc: 0.7363 - val_loss: 0.0018 - val_acc: 0.7143\n",
      "Epoch 21/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0058 - acc: 0.7404\n",
      "Epoch 00021: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0053 - acc: 0.7363 - val_loss: 0.0017 - val_acc: 0.7143\n",
      "Epoch 22/200\n",
      "68/81 [========================>.....] - ETA: 0s - loss: 0.0042 - acc: 0.7500\n",
      "Epoch 00022: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0052 - acc: 0.7363 - val_loss: 0.0016 - val_acc: 0.7143\n",
      "Epoch 23/200\n",
      "52/81 [==================>...........] - ETA: 0s - loss: 0.0041 - acc: 0.7356\n",
      "Epoch 00023: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0051 - acc: 0.7363 - val_loss: 0.0015 - val_acc: 0.7143\n",
      "Epoch 24/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.0057 - acc: 0.7220\n",
      "Epoch 00024: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0050 - acc: 0.7363 - val_loss: 0.0015 - val_acc: 0.7143\n",
      "Epoch 25/200\n",
      "63/81 [======================>.......] - ETA: 0s - loss: 0.0056 - acc: 0.7361\n",
      "Epoch 00025: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0050 - acc: 0.7363 - val_loss: 0.0014 - val_acc: 0.7143\n",
      "Epoch 26/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0057 - acc: 0.7292\n",
      "Epoch 00026: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0049 - acc: 0.7363 - val_loss: 0.0014 - val_acc: 0.7143\n",
      "Epoch 27/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0037 - acc: 0.7500\n",
      "Epoch 00027: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0049 - acc: 0.7363 - val_loss: 0.0013 - val_acc: 0.7143\n",
      "Epoch 28/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.0036 - acc: 0.7500\n",
      "Epoch 00028: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0048 - acc: 0.7363 - val_loss: 0.0013 - val_acc: 0.7143\n",
      "Epoch 29/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0054 - acc: 0.7386\n",
      "Epoch 00029: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0047 - acc: 0.7363 - val_loss: 0.0013 - val_acc: 0.7143\n",
      "Epoch 30/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.0052 - acc: 0.7575\n",
      "Epoch 00030: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0046 - acc: 0.7363 - val_loss: 0.0012 - val_acc: 0.7143\n",
      "Epoch 31/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0050 - acc: 0.7386\n",
      "Epoch 00031: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0045 - acc: 0.7363 - val_loss: 0.0012 - val_acc: 0.7143\n",
      "Epoch 32/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0052 - acc: 0.7405\n",
      "Epoch 00032: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0045 - acc: 0.7363 - val_loss: 0.0012 - val_acc: 0.7143\n",
      "Epoch 33/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0049 - acc: 0.7462\n",
      "Epoch 00033: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0044 - acc: 0.7363 - val_loss: 0.0012 - val_acc: 0.7143\n",
      "Epoch 34/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.0034 - acc: 0.7351\n",
      "Epoch 00034: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0043 - acc: 0.7363 - val_loss: 0.0012 - val_acc: 0.7143\n",
      "Epoch 35/200\n",
      "64/81 [======================>.......] - ETA: 0s - loss: 0.0051 - acc: 0.7305\n",
      "Epoch 00035: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0043 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 36/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0030 - acc: 0.7367\n",
      "Epoch 00036: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0043 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 37/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0048 - acc: 0.7327\n",
      "Epoch 00037: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0043 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 38/200\n",
      "64/81 [======================>.......] - ETA: 0s - loss: 0.0034 - acc: 0.7305\n",
      "Epoch 00038: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0043 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 39/200\n",
      "70/81 [========================>.....] - ETA: 0s - loss: 0.0045 - acc: 0.7304\n",
      "Epoch 00039: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 40/200\n",
      "69/81 [========================>.....] - ETA: 0s - loss: 0.0048 - acc: 0.7446\n",
      "Epoch 00040: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 41/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0017 - acc: 0.7327\n",
      "Epoch 00041: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 42/200\n",
      "62/81 [=====================>........] - ETA: 0s - loss: 0.0031 - acc: 0.7319\n",
      "Epoch 00042: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 43/200\n",
      "64/81 [======================>.......] - ETA: 0s - loss: 0.0050 - acc: 0.7344\n",
      "Epoch 00043: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 44/200\n",
      "68/81 [========================>.....] - ETA: 0s - loss: 0.0028 - acc: 0.7426\n",
      "Epoch 00044: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 45/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0048 - acc: 0.7424\n",
      "Epoch 00045: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 46/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0050 - acc: 0.7159\n",
      "Epoch 00046: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 47/200\n",
      "64/81 [======================>.......] - ETA: 0s - loss: 0.0030 - acc: 0.7461\n",
      "Epoch 00047: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 48/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0047 - acc: 0.7288\n",
      "Epoch 00048: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 49/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0049 - acc: 0.7308\n",
      "Epoch 00049: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 50/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0032 - acc: 0.7576\n",
      "Epoch 00050: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 51/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0032 - acc: 0.7254\n",
      "Epoch 00051: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 52/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0049 - acc: 0.7348\n",
      "Epoch 00052: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0011 - val_acc: 0.7143\n",
      "Epoch 53/200\n",
      "62/81 [=====================>........] - ETA: 0s - loss: 0.0051 - acc: 0.7258\n",
      "Epoch 00053: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 54/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0049 - acc: 0.7385\n",
      "Epoch 00054: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 55/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.0048 - acc: 0.7295\n",
      "Epoch 00055: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 56/200\n",
      "69/81 [========================>.....] - ETA: 0s - loss: 0.0045 - acc: 0.7264\n",
      "Epoch 00056: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 57/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.0032 - acc: 0.7369\n",
      "Epoch 00057: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 58/200\n",
      "67/81 [=======================>......] - ETA: 0s - loss: 0.0049 - acc: 0.7183\n",
      "Epoch 00058: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 59/200\n",
      "68/81 [========================>.....] - ETA: 0s - loss: 0.0049 - acc: 0.7371\n",
      "Epoch 00059: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 60/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0034 - acc: 0.7346\n",
      "Epoch 00060: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 61/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0015 - acc: 0.7386\n",
      "Epoch 00061: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 62/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0049 - acc: 0.7423\n",
      "Epoch 00062: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 63/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0050 - acc: 0.7423\n",
      "Epoch 00063: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 64/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0049 - acc: 0.7273\n",
      "Epoch 00064: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 65/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0049 - acc: 0.7385\n",
      "Epoch 00065: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 66/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0047 - acc: 0.7386\n",
      "Epoch 00066: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 67/200\n",
      "68/81 [========================>.....] - ETA: 0s - loss: 0.0047 - acc: 0.7316\n",
      "Epoch 00067: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 68/200\n",
      "68/81 [========================>.....] - ETA: 0s - loss: 0.0033 - acc: 0.7316    \n",
      "Epoch 00068: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 69/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0047 - acc: 0.7481\n",
      "Epoch 00069: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 70/200\n",
      "66/81 [=======================>......] - ETA: 0s - loss: 0.0047 - acc: 0.7311\n",
      "Epoch 00070: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 71/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0049 - acc: 0.7365\n",
      "Epoch 00071: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 72/200\n",
      "64/81 [======================>.......] - ETA: 0s - loss: 0.0050 - acc: 0.7383\n",
      "Epoch 00072: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 73/200\n",
      "65/81 [=======================>......] - ETA: 0s - loss: 0.0049 - acc: 0.7327\n",
      "Epoch 00073: val_acc did not improve from 0.71429\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 0.7363 - val_loss: 0.0010 - val_acc: 0.7143\n",
      "Epoch 00073: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['acc'])\n",
    "history = model.fit(x_train, y_train, epochs = 200, batch_size = 8, validation_data = (x_valid, y_valid), callbacks = [es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T07:08:11.196238Z",
     "iopub.status.busy": "2021-01-13T07:08:11.195542Z",
     "iopub.status.idle": "2021-01-13T07:08:11.437837Z",
     "shell.execute_reply": "2021-01-13T07:08:11.437245Z"
    },
    "papermill": {
     "duration": 0.359257,
     "end_time": "2021-01-13T07:08:11.437962",
     "exception": false,
     "start_time": "2021-01-13T07:08:11.078705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check result on graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "acc = history.history['acc']\n",
    "val_loss = history.history['val_loss']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(loss, label= 'loss')\n",
    "plt.plot(acc, label= 'acc')\n",
    "plt.plot(val_loss, label= 'val_loss')\n",
    "plt.plot(val_acc, label= 'val_acc')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "duration": 52.196159,
   "end_time": "2021-01-13T07:08:11.657082",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-13T07:07:19.460923",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
