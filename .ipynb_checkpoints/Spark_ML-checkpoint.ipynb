{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, StandardScaler, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Keras / Deep Learning\n",
    "\"\"\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import optimizers, regularizers\n",
    "from keras.optimizers import Adam\n",
    "\"\"\"\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector,GRU, Input, ConvLSTM2D, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# Elephas for Deep Learning on Spark\n",
    "from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------------+-------+\n",
      "|      Date|    time|Allocate memory|  %used|\n",
      "+----------+--------+---------------+-------+\n",
      "|2020/11/30|23:57:45|     1606751865|31.6945|\n",
      "|2020/11/30|23:54:46|     1606751686|31.9328|\n",
      "|2020/11/30|23:51:45|     1606751505|32.8929|\n",
      "|2020/11/30|23:48:45|     1606751325|34.5618|\n",
      "|2020/11/30|23:45:45|     1606751145|  32.14|\n",
      "+----------+--------+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 1.81 ms, sys: 2.72 ms, total: 4.53 ms\n",
      "Wall time: 798 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.csv(\"hdfs://master:9000/user/data/ATKH_Oplus_TWGKHHPSK1MSB04_memory_usage_2020_11.csv\", inferSchema=True, header=True)\n",
    "# inferSchema referring to the type of the column\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------------+-------+\n",
      "|      Date|    time|Allocate memory|  %used|\n",
      "+----------+--------+---------------+-------+\n",
      "|2020/11/30|23:57:45|     1606751865|31.6945|\n",
      "|2020/11/30|23:54:46|     1606751686|31.9328|\n",
      "|2020/11/30|23:51:45|     1606751505|32.8929|\n",
      "|2020/11/30|23:48:45|     1606751325|34.5618|\n",
      "|2020/11/30|23:45:45|     1606751145|  32.14|\n",
      "+----------+--------+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to select features to scale given their skew\n",
    "def select_features_to_scale(df=df, drop_cols=['Date','time','Allocate memory']):\n",
    "    \n",
    "    # Empty Selected Feature List for Output\n",
    "    selected_features = []\n",
    "    \n",
    "    # Select Features to Scale based on Inputs ('in32' type, drop 'ID' columns or others, skew bounds)\n",
    "    feature_list = list(df.toPandas().columns.drop(drop_cols))\n",
    "    \n",
    "    # Loop through 'feature_list' to select features based on Kurtosis / Skew\n",
    "    for feature in feature_list:\n",
    "\n",
    "        selected_features.append(feature)\n",
    "        #if df.toPandas()[feature].kurtosis() < -2 or df.toPandas()[feature].kurtosis() > 2:\n",
    "            \n",
    "            \n",
    "    \n",
    "    # Return feature list to scale\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "\n",
    "unscaled_features = select_features_to_scale(df=df)\n",
    "unscaled_assembler = VectorAssembler(inputCols=unscaled_features, outputCol=\"unscaled_features\")\n",
    "scaler = MinMaxScaler(min=0.0, max=1.0, inputCol=\"unscaled_features\", outputCol=\"scaled_features\")\n",
    "\n",
    "\n",
    "\n",
    "stages += [unscaled_assembler, scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler_495b8494aa53"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit Pipeline to Data\n",
    "pipeline_model = pipeline.fit(df)\n",
    "\n",
    "# Transform Data using Fitted Pipeline\n",
    "df_transform = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [0.20730249328605418]\n",
       "1     [0.21053631501739048]\n",
       "2      [0.2235652375698705]\n",
       "3     [0.24621284599382007]\n",
       "4     [0.21334809790758305]\n",
       "5     [0.23519098275340317]\n",
       "6      [0.2083799815171415]\n",
       "7     [0.20912499541999655]\n",
       "8      [0.2021525337936407]\n",
       "9     [0.20112932708553008]\n",
       "10    [0.19960401628988506]\n",
       "11    [0.23043185022642182]\n",
       "12    [0.23969363508431962]\n",
       "13    [0.23858086386329747]\n",
       "14    [0.23092309800936087]\n",
       "15    [0.23222585456080147]\n",
       "16    [0.20034903019274014]\n",
       "17    [0.20299389739977935]\n",
       "18    [0.20375655279760188]\n",
       "19    [0.20208739596606864]\n",
       "20    [0.20877216552064806]\n",
       "21    [0.21762412488007182]\n",
       "22     [0.2248951348827995]\n",
       "23    [0.21484626794173967]\n",
       "24    [0.20553970082738612]\n",
       "25    [0.21209283768874707]\n",
       "26    [0.21817643937635967]\n",
       "27    [0.21848584405732674]\n",
       "28    [0.21325310524237387]\n",
       "29    [0.21331960010802029]\n",
       "30    [0.21578805236538526]\n",
       "31    [0.22987003646361312]\n",
       "32     [0.2153008756966695]\n",
       "33    [0.21867447234967075]\n",
       "34    [0.21599025103847339]\n",
       "35     [0.2586718125550449]\n",
       "36    [0.20292875957220735]\n",
       "37     [0.2031146737884025]\n",
       "38     [0.2152113111837579]\n",
       "39    [0.21923764315055388]\n",
       "40    [0.21845463218161518]\n",
       "41     [0.2285632087979493]\n",
       "42     [0.2356116645564725]\n",
       "43    [0.21680583092119815]\n",
       "44    [0.18082396637802473]\n",
       "45     [0.1795361372454027]\n",
       "46    [0.27123934216222306]\n",
       "47     [0.2717455173639807]\n",
       "48     [0.2144418705955633]\n",
       "49    [0.20605130418144146]\n",
       "50    [0.19854281251569078]\n",
       "51    [0.19976686085881515]\n",
       "52    [0.19966372596515944]\n",
       "53    [0.20583824920375796]\n",
       "54    [0.21276457153558362]\n",
       "55    [0.21666198488530997]\n",
       "56    [0.21601196364766412]\n",
       "57    [0.19934210794152257]\n",
       "58    [0.19986728167632203]\n",
       "59    [0.19693065128328308]\n",
       "Name: scaled_features, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transform.limit(60).toPandas()['scaled_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform_fin = df_transform.select('scaled_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[scaled_features: vector]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transform_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-0b8cbb771dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mae\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_input_dim1 = 60\n",
    "X_train_input_dim2 = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(60, input_shape=(X_train_input_dim1, X_train_input_dim2),return_sequences=True,unroll=True,activation='relu'))\n",
    "\n",
    "model.add(LSTM(60, return_sequences=False,unroll=True,activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
