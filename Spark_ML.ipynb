{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, StandardScaler, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Keras / Deep Learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import optimizers, regularizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Elephas for Deep Learning on Spark\n",
    "from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------------+-------+\n",
      "|      Date|    time|Allocate memory|  %used|\n",
      "+----------+--------+---------------+-------+\n",
      "|2020/11/30|23:57:45|     1606751865|31.6945|\n",
      "|2020/11/30|23:54:46|     1606751686|31.9328|\n",
      "|2020/11/30|23:51:45|     1606751505|32.8929|\n",
      "|2020/11/30|23:48:45|     1606751325|34.5618|\n",
      "|2020/11/30|23:45:45|     1606751145|  32.14|\n",
      "+----------+--------+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 4.22 ms, sys: 4.28 ms, total: 8.5 ms\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.csv(\"hdfs://master:9000/user/data/ATKH_Oplus_TWGKHHPSK1MSB04_memory_usage_2020_11.csv\", inferSchema=True, header=True)\n",
    "# inferSchema referring to the type of the column\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Allocate memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+\n",
      "|      Date|    time|  %used|\n",
      "+----------+--------+-------+\n",
      "|2020/11/30|23:57:45|31.6945|\n",
      "|2020/11/30|23:54:46|31.9328|\n",
      "|2020/11/30|23:51:45|32.8929|\n",
      "|2020/11/30|23:48:45|34.5618|\n",
      "|2020/11/30|23:45:45|  32.14|\n",
      "+----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to select features to scale given their skew\n",
    "def select_features_to_scale(df=df, lower_skew=-2, upper_skew=2, drop_cols=['Date','time']):\n",
    "    \n",
    "    # Empty Selected Feature List for Output\n",
    "    selected_features = []\n",
    "    \n",
    "    # Select Features to Scale based on Inputs ('in32' type, drop 'ID' columns or others, skew bounds)\n",
    "    feature_list = list(df.toPandas().columns.drop(drop_cols))\n",
    "    \n",
    "    # Loop through 'feature_list' to select features based on Kurtosis / Skew\n",
    "    for feature in feature_list:\n",
    "\n",
    "        selected_features.append(feature)\n",
    "        #if df.toPandas()[feature].kurtosis() < -2 or df.toPandas()[feature].kurtosis() > 2:\n",
    "            \n",
    "            \n",
    "    \n",
    "    # Return feature list to scale\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "\n",
    "unscaled_features = select_features_to_scale(df=df, lower_skew=-2, upper_skew=2)\n",
    "unscaled_assembler = VectorAssembler(inputCols=unscaled_features, outputCol=\"unscaled_features\")\n",
    "scaler = MinMaxScaler(min=0.0, max=1.0, inputCol=\"unscaled_features\", outputCol=\"scaled_features\")\n",
    "stages += [unscaled_assembler, scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit Pipeline to Data\n",
    "pipeline_model = pipeline.fit(df)\n",
    "\n",
    "# Transform Data using Fitted Pipeline\n",
    "df_transform = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.20730249328605418]\n",
       "1    [0.21053631501739048]\n",
       "2     [0.2235652375698705]\n",
       "3    [0.24621284599382007]\n",
       "4    [0.21334809790758305]\n",
       "Name: scaled_features, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transform.limit(5).toPandas()['scaled_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
